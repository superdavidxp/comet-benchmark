#!/bin/bash

#SBATCH --account=ddp315
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --time 00:10:00
#SBATCH --export=ALL
#SBATCH --no-requeue
#SBATCH --job-name="tensorflow-cifar10-gpu"
#SBATCH --output="tensorflow-cifar10-gpu.o%j.%N"
#SBATCH --error="tensorflow-cifar10-gpu.e%j.%N"
#SBATCH --gres=gpu:k80:4

declare -xr LOCAL_SCRATCH="/scratch/${USER}/${SLURM_JOB_ID}"
declare -xr LUSTRE_SCRATCH="/home/dmu/SCRATCH/SINGULARITY/images"
declare -xr SINGULARITY_MODULE='singularity/2.5.2'

module purge
module load gnu
module load mvapich2_ib
module load cmake
module load "${SINGULARITY_MODULE}"
module list

cp -rf ../../../../comet-benchmark "${LOCAL_SCRATCH}"
cp "${LUSTRE_SCRATCH}/tensorflow-gpu-mpi.simg" "${LOCAL_SCRATCH}"

cd "${LOCAL_SCRATCH}/comet-benchmark/singularity/tensorflow/MPI2"

node_list=($(scontrol show hostnames))

echo ${node_list[0]}
echo ${node_list[1]}
echo ${node_list[2]}

#
# singularity exec --nv ${LOCAL_SCRATCH}/tensorflow-gpu-mpi.simg mpiexec -np 4 /usr/bin/python3 ~/SCRATCH/comet-benchmark/singularity/tensorflow/DistributedTF/main_manager.py
# singularity exec --nv ~/SCRATCH/SINGULARITY/images/tensorflow-gpu-mpi.simg python3 ~/SCRATCH/comet-benchmark/singularity/tensorflow/MPI2/main_manager.py
mpirun -np 4 singularity exec --nv ~/SCRATCH/SINGULARITY/images/tensorflow-gpu-mpi.simg sh /home/dmu/SCRATCH/comet-benchmark/singularity/tensorflow/MPI2/run_loc.sh
