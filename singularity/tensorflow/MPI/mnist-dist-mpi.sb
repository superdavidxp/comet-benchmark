#!/bin/bash

#SBATCH --account=ddp315
#SBATCH --partition=gpu
#SBATCH --nodes=3
#SBATCH --ntasks-per-node=1
#SBATCH --time 00:10:00
#SBATCH --export=ALL
#SBATCH --no-requeue
#SBATCH --job-name="tensorflow-cifar10-gpu"
#SBATCH --output="tensorflow-cifar10-gpu.o%j.%N"
#SBATCH --error="tensorflow-cifar10-gpu.e%j.%N"
#SBATCH --gres=gpu:k80:4

declare -xr LOCAL_SCRATCH="/scratch/${USER}/${SLURM_JOB_ID}"
declare -xr LUSTRE_SCRATCH="/home/dmu/SCRATCH/SINGULARITY/images"
declare -xr SINGULARITY_MODULE='singularity/2.5.2'

module purge
module load gnu
module load mvapich2_ib
module load cmake
module load "${SINGULARITY_MODULE}"
module list

cp -rf ../../../../comet-benchmark "${LOCAL_SCRATCH}"
cp "${LUSTRE_SCRATCH}/tensorflow-gpu-mpi.simg" "${LOCAL_SCRATCH}"

cd "${LOCAL_SCRATCH}/comet-benchmark/singularity/tensorflow/MPI"

node_list=($(scontrol show hostnames $SLURM_JOB_NODELIST))

echo ${node_list[0]}
echo ${node_list[1]}
echo ${node_list[2]}

#
singularity exec --nv ${LOCAL_SCRATCH}/tensorflow-gpu-mpi.simg mpiexec -ppn 1 python -u \
                cluster_dispatch.py \
                --ps_hosts=${node_list[0]}:2222 --worker_hosts=${node_list[1]}:2222,${node_list[2]}:2222 \
                --script=example.py
